---
layout: post
collection: til
description: >
  2025-05-20 TIL
categories: ["til"]
tags: ["TIL"]
date: 2025-05-20 00:00:00
last_modified_at: 2025-05-22 05:33:07
github_issue: 57
github_url: https://github.com/nan0silver/TIL/issues/57
sitemap: false
---

# [TIL] Ollama

> 📝 **TIL (Today I Learned)**  
> 🔗 **원본 이슈**: [#57](https://github.com/nan0silver/TIL/issues/57)  
> 📅 **작성일**: 2025-05-20  
> 🔄 **최종 수정**: 2025년 05월 22일

---


## 🍀 새롭게 배운 것

- Ollama

  - 로컬 머신에서 대규모 언어 모델(LLM)을 실행할 수 있게 해주는 오픈소스 도구
  - 주요 특징
    - 로컬 실행: 모든 처리가 로컬에서 이루어져 개인정보 보호가 가능
    - 다양한 모델 지원: Llama, Mistral, Gemma 등 다양한 오픈소스 모델 지원
    - 간단한 API: REST API를 통해 쉽게 통합 가능
    - CPU/GPU 지원: 다양한 하드웨어에서 실행 가능
    - 크로스 플랫폼: Windows, macOS, Linux 지원
  - Ollama 시작하기
    1. ollama.com에서 OS에 맞는 버전 다운로드 및 설치
    2. 터미널에서 모델 다운로드 (예: ollama pull llama3.2)
    3. 모델 실행 (예: ollama run llama3.2)
  - API 사용 방법

    - Ollama는 http://localhost:11434에서 REST API를 제공합니다:
    - ```python
        # Python 예제
        import requests

        response = requests.post('http://localhost:11434/api/generate',
            json={
                'model': 'llama3.2',
                'prompt': '백엔드 개발에 필요한 기술은?',
                'stream': False
            })

        print(response.json()['response'])
      ```

    - ```javascript
      // JavaScript 예제
      fetch("http://localhost:11434/api/generate", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          model: "llama3.2",
          prompt: "백엔드 개발에 필요한 기술은?",
          stream: false,
        }),
      })
        .then((response) => response.json())
        .then((data) => console.log(data.response));
      ```
